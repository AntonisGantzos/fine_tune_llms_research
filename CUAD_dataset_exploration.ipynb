{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "08b34bbd",
      "metadata": {},
      "source": [
        "\n",
        "**Contract Understanding Atticus Dataset (CUAD) v1** — Expert-annotated NLP dataset for legal contract review (NeurIPS 2021).\n",
        "\n",
        "This notebook provides a step-by-step analysis of every aspect of the CUAD dataset, aligned with the research goals in the project README:\n",
        "- **Task 1 (T1):** Risk Clause Recognition — identify specific risk clauses (e.g., Termination for Convenience, Uncapped Liability)\n",
        "- **Task 2 (T2):** Structured Entity Extraction — extract entities like Agreement Date, Parties, Renewal Term as valid JSON\n",
        "\n",
        "**Dataset Summary:** 13,000+ expert annotations across 510 commercial contracts, 41 clause categories, 25 contract types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "07b2c031",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading to C:\\Users\\aetho\\.cache\\kagglehub\\datasets\\theatticusproject\\atticus-open-contract-dataset-aok-beta\\3.archive...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8.70k/8.70k [00:00<00:00, 7.97MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: C:\\Users\\aetho\\.cache\\kagglehub\\datasets\\theatticusproject\\atticus-open-contract-dataset-aok-beta\\versions\\3\n",
            "Loading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup and Load Dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "# Load CUAD from Hugging Face\n",
        "try:\n",
        "    # Download latest version\n",
        "    path = kagglehub.dataset_download(\"theatticusproject/atticus-open-contract-dataset-aok-beta\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "    print(\"Loading dataset...\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096414c6",
      "metadata": {},
      "source": [
        "## Step 2: Dataset Structure & Schema\n",
        "\n",
        "Explore the data schema, field types, and splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72648fb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema and splits\n",
        "print(\"=== Dataset Schema ===\\n\")\n",
        "print(cuad[\"train\"].features)\n",
        "print(\"\\n=== Split Sizes ===\")\n",
        "for split in cuad.keys():\n",
        "    print(f\"  {split}: {len(cuad[split]):,} samples\")\n",
        "print(f\"\\n  Total: {sum(len(cuad[s]) for s in cuad):,} samples\")\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "train_df = pd.DataFrame(cuad[\"train\"])\n",
        "test_df = pd.DataFrame(cuad[\"test\"])\n",
        "print(\"\\n=== Sample Record (Train) ===\")\n",
        "sample = train_df.iloc[0]\n",
        "for col in train_df.columns:\n",
        "    val = sample[col]\n",
        "    if isinstance(val, str) and len(val) > 100:\n",
        "        val = val[:100] + \"...\"\n",
        "    print(f\"  {col}: {val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2ebe5b",
      "metadata": {},
      "source": [
        "Extract and analyze the 41 CUAD clause categories from the `question` field. Map to research tasks:\n",
        "- **T1 (Clause Recognition):** Yes/No categories — Non-Compete, Exclusivity, Termination for Convenience, Uncapped Liability, etc.\n",
        "- **T2 (Entity Extraction):** Entity categories — Document Name, Parties, Agreement Date, Effective Date, Renewal Term, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9839e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract clause category from question (format: ...related to \"Category Name\" that should be...)\n",
        "def extract_category(q):\n",
        "    m = re.search(r'related to \"([^\"]+)\" that should be', str(q))\n",
        "    return m.group(1) if m else \"unknown\"\n",
        "\n",
        "for df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n",
        "    df[\"category\"] = df[\"question\"].apply(extract_category)\n",
        "\n",
        "# Combine for full dataset stats\n",
        "all_categories_train = train_df[\"category\"].value_counts()\n",
        "all_categories_test = test_df[\"category\"].value_counts()\n",
        "\n",
        "print(\"=== 41 Clause Categories (Train) ===\\n\")\n",
        "print(all_categories_train.to_string())\n",
        "print(f\"\\nUnique categories (train): {train_df['category'].nunique()}\")\n",
        "print(f\"Unique categories (test): {test_df['category'].nunique()}\")\n",
        "\n",
        "# Entity vs Yes/No categories (for T1 vs T2)\n",
        "ENTITY_CATEGORIES = {\"Document Name\", \"Parties\", \"Agreement Date\", \"Effective Date\", \"Expiration Date\", \n",
        "                     \"Renewal Term\", \"Notice Period to Terminate Renewal\", \"Governing Law\", \"Warranty Duration\"}\n",
        "train_df[\"task_type\"] = train_df[\"category\"].apply(lambda c: \"Entity (T2)\" if c in ENTITY_CATEGORIES else \"Clause (T1)\")\n",
        "test_df[\"task_type\"] = test_df[\"category\"].apply(lambda c: \"Entity (T2)\" if c in ENTITY_CATEGORIES else \"Clause (T1)\")\n",
        "\n",
        "print(\"\\n=== Task Type Distribution (Train) ===\")\n",
        "print(train_df[\"task_type\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c9a9901",
      "metadata": {},
      "source": [
        "## Step 4: Contract Types (25 Types)\n",
        "\n",
        "CUAD includes 25 contract types (e.g., Distributor Agreement, License Agreement, Service Agreement). Extract from `title` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e89ea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract contract type from title (format: COMPANY_DATE-EX-NUM-CONTRACT TYPE)\n",
        "def extract_contract_type(title):\n",
        "    parts = str(title).split(\"-\")\n",
        "    return parts[-1].strip() if len(parts) > 1 else \"unknown\"\n",
        "\n",
        "train_df[\"contract_type\"] = train_df[\"title\"].apply(extract_contract_type)\n",
        "test_df[\"contract_type\"] = test_df[\"title\"].apply(extract_contract_type)\n",
        "\n",
        "print(\"=== Contract Types (Train) - Top 20 ===\")\n",
        "print(train_df[\"contract_type\"].value_counts().head(20).to_string())\n",
        "print(f\"\\nUnique contract types: {train_df['contract_type'].nunique()}\")\n",
        "print(f\"Unique contracts (titles): {train_df['title'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63cfc6eb",
      "metadata": {},
      "source": [
        "## Step 5: Answer & Entity Analysis\n",
        "\n",
        "Analyze the `answers` field: text extracted, answer_start positions, and whether answers are empty (no clause found)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e143e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Answer analysis\n",
        "def get_answer_text(answers):\n",
        "    txt = answers.get(\"text\", [])\n",
        "    return txt[0] if txt else \"\"\n",
        "\n",
        "def get_answer_start(answers):\n",
        "    starts = answers.get(\"answer_start\", [])\n",
        "    return starts[0] if starts else -1\n",
        "\n",
        "def is_empty_answer(answers):\n",
        "    txt = answers.get(\"text\", [])\n",
        "    return len(txt) == 0 or (len(txt) == 1 and (txt[0] == \"\" or txt[0].strip() == \"\"))\n",
        "\n",
        "train_df[\"answer_text\"] = train_df[\"answers\"].apply(get_answer_text)\n",
        "train_df[\"answer_start\"] = train_df[\"answers\"].apply(get_answer_start)\n",
        "train_df[\"empty_answer\"] = train_df[\"answers\"].apply(is_empty_answer)\n",
        "\n",
        "test_df[\"answer_text\"] = test_df[\"answers\"].apply(get_answer_text)\n",
        "test_df[\"answer_start\"] = test_df[\"answers\"].apply(get_answer_start)\n",
        "test_df[\"empty_answer\"] = test_df[\"answers\"].apply(is_empty_answer)\n",
        "\n",
        "print(\"=== Answer Statistics (Train) ===\")\n",
        "print(f\"Empty answers (no clause/entity found): {train_df['empty_answer'].sum():,} ({100*train_df['empty_answer'].mean():.1f}%)\")\n",
        "print(f\"Non-empty answers: {(~train_df['empty_answer']).sum():,}\")\n",
        "\n",
        "# Answer length distribution (non-empty)\n",
        "non_empty = train_df[~train_df[\"empty_answer\"]]\n",
        "lens = non_empty[\"answer_text\"].str.len()\n",
        "print(f\"\\nAnswer text length (non-empty): min={lens.min()}, max={lens.max()}, median={lens.median():.0f}, mean={lens.mean():.1f}\")\n",
        "\n",
        "# Empty rate by category\n",
        "empty_by_cat = train_df.groupby(\"category\")[\"empty_answer\"].agg([\"sum\", \"count\", \"mean\"])\n",
        "empty_by_cat[\"pct_empty\"] = (100 * empty_by_cat[\"mean\"]).round(1)\n",
        "print(\"\\n=== Empty Answer Rate by Category (top 15 highest) ===\")\n",
        "print(empty_by_cat.nlargest(15, \"mean\")[[\"sum\", \"count\", \"pct_empty\"]].to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87892802",
      "metadata": {},
      "source": [
        "## Step 6: Context (Contract Text) Analysis\n",
        "\n",
        "Contract length affects fine-tuning: longer contexts require more memory and may need truncation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0319d73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Context length (characters and tokens approximation: ~4 chars/token)\n",
        "train_df[\"context_len_char\"] = train_df[\"context\"].str.len()\n",
        "train_df[\"context_len_tokens_approx\"] = (train_df[\"context_len_char\"] / 4).astype(int)\n",
        "\n",
        "print(\"=== Context Length Statistics (Train) ===\")\n",
        "print(train_df[\"context_len_char\"].describe().to_string())\n",
        "print(f\"\\nApprox tokens: min={train_df['context_len_tokens_approx'].min()}, max={train_df['context_len_tokens_approx'].max()}, median={train_df['context_len_tokens_approx'].median():.0f}\")\n",
        "\n",
        "# Percentiles\n",
        "for p in [50, 90, 95, 99, 100]:\n",
        "    v = train_df[\"context_len_char\"].quantile(p/100)\n",
        "    print(f\"  {p}th percentile: {v:,.0f} chars (~{v/4:.0f} tokens)\")\n",
        "\n",
        "# Contracts exceeding common context limits\n",
        "limits = [512, 1024, 2048, 4096, 8192]\n",
        "for lim in limits:\n",
        "    exceed = (train_df[\"context_len_tokens_approx\"] > lim).sum()\n",
        "    print(f\"\\nExceeding {lim} tokens: {exceed:,} samples ({100*exceed/len(train_df):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d64a781",
      "metadata": {},
      "source": [
        "## Step 7: Data Quality Checks\n",
        "\n",
        "Verify schema consistency, nulls, and common pitfalls for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d0ec2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality\n",
        "print(\"=== Null Check (Train) ===\")\n",
        "print(train_df.isnull().sum().to_string())\n",
        "\n",
        "print(\"\\n=== Duplicate IDs ===\")\n",
        "dup_ids = train_df[train_df.duplicated(subset=[\"id\"], keep=False)]\n",
        "print(f\"Duplicate IDs: {dup_ids['id'].nunique()} unique IDs with duplicates, {len(dup_ids)} rows\")\n",
        "\n",
        "print(\"\\n=== Answer Format Consistency ===\")\n",
        "# Check answer_start aligns with context\n",
        "def answer_in_context(row):\n",
        "    if row[\"empty_answer\"]:\n",
        "        return True\n",
        "    txt = row[\"answer_text\"]\n",
        "    start = row[\"answer_start\"]\n",
        "    ctx = row[\"context\"]\n",
        "    if start < 0 or start >= len(ctx):\n",
        "        return False\n",
        "    return ctx[start:start+len(txt)] == txt\n",
        "\n",
        "align_check = train_df[~train_df[\"empty_answer\"]].apply(answer_in_context, axis=1)\n",
        "print(f\"Answer spans correctly in context: {align_check.sum()}/{len(align_check)} ({100*align_check.mean():.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0f94b4",
      "metadata": {},
      "source": [
        "## Step 8: Visualizations\n",
        "\n",
        "Charts for clause distribution, contract types, answer lengths, and context lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647aaad7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Top 20 clause categories\n",
        "cat_counts = train_df[\"category\"].value_counts().head(20)\n",
        "axes[0,0].barh(range(len(cat_counts)), cat_counts.values)\n",
        "axes[0,0].set_yticks(range(len(cat_counts)))\n",
        "axes[0,0].set_yticklabels(cat_counts.index, fontsize=8)\n",
        "axes[0,0].invert_yaxis()\n",
        "axes[0,0].set_title(\"Top 20 Clause Categories (Train)\")\n",
        "axes[0,0].set_xlabel(\"Count\")\n",
        "\n",
        "# 2. Empty vs non-empty answers by task type\n",
        "task_empty = train_df.groupby(\"task_type\")[\"empty_answer\"].value_counts().unstack(fill_value=0)\n",
        "task_empty.plot(kind=\"bar\", ax=axes[0,1], stacked=True, color=[\"#2ecc71\", \"#e74c3c\"])\n",
        "axes[0,1].set_title(\"Answer Presence by Task Type (T1 vs T2)\")\n",
        "axes[0,1].set_ylabel(\"Count\")\n",
        "axes[0,1].legend([\"Has answer\", \"Empty\"])\n",
        "axes[0,1].tick_params(axis=\"x\", rotation=0)\n",
        "\n",
        "# 3. Context length distribution\n",
        "axes[1,0].hist(train_df[\"context_len_char\"] / 1000, bins=50, edgecolor=\"black\", alpha=0.7)\n",
        "axes[1,0].set_title(\"Context Length Distribution (chars, in thousands)\")\n",
        "axes[1,0].set_xlabel(\"Context length (k chars)\")\n",
        "axes[1,0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# 4. Top 15 contract types\n",
        "ct = train_df[\"contract_type\"].value_counts().head(15)\n",
        "axes[1,1].barh(range(len(ct)), ct.values)\n",
        "axes[1,1].set_yticks(range(len(ct)))\n",
        "axes[1,1].set_yticklabels(ct.index, fontsize=8)\n",
        "axes[1,1].invert_yaxis()\n",
        "axes[1,1].set_title(\"Top 15 Contract Types (Train)\")\n",
        "axes[1,1].set_xlabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395f0a14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Answer length distribution (non-empty only)\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "lens = train_df[~train_df[\"empty_answer\"]][\"answer_text\"].str.len()\n",
        "ax.hist(lens.clip(upper=500), bins=50, edgecolor=\"black\", alpha=0.7)\n",
        "ax.set_title(\"Answer Text Length (non-empty, capped at 500 chars)\")\n",
        "ax.set_xlabel(\"Character length\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e899387",
      "metadata": {},
      "source": [
        "## Step 9: Summary & Implications for Fine-Tuning\n",
        "\n",
        "Key takeaways from the exploration for LoRA/QLoRA fine-tuning (as per project README):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd4f02af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "summary = {\n",
        "    \"Total train samples\": len(train_df),\n",
        "    \"Total test samples\": len(test_df),\n",
        "    \"Unique clause categories\": train_df[\"category\"].nunique(),\n",
        "    \"Unique contracts (titles)\": train_df[\"title\"].nunique(),\n",
        "    \"Empty answer rate (%)\": round(100 * train_df[\"empty_answer\"].mean(), 1),\n",
        "    \"Median context length (chars)\": int(train_df[\"context_len_char\"].median()),\n",
        "    \"Entity (T2) samples\": (train_df[\"task_type\"] == \"Entity (T2)\").sum(),\n",
        "    \"Clause (T1) samples\": (train_df[\"task_type\"] == \"Clause (T1)\").sum(),\n",
        "}\n",
        "print(\"=== CUAD Exploration Summary ===\\n\")\n",
        "for k, v in summary.items():\n",
        "    print(f\"  {k}: {v:,}\" if isinstance(v, int) else f\"  {k}: {v}\")\n",
        "print(\"\\nImplications:\")\n",
        "print(\"  • T1 (Clause Recognition): Yes/No + span extraction; many empty answers.\")\n",
        "print(\"  • T2 (Entity Extraction): Structured entities (dates, parties); output as JSON.\")\n",
        "print(\"  • Context truncation: Most contracts exceed 512 tokens; 4096+ recommended.\")\n",
        "print(\"  • Imbalance: Category distribution varies; consider sampling for training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a9b294",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myvenv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
